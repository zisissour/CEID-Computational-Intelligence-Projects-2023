----QA1_1----

Normalization -> Καλή όταν δε ξέρουμε την κατανομή των δεδομένω ή όταν αυτή δεν είναι Gaussian. Μειώνουμε την επίδραση των ακραίων τιμών.

Standarization -> Καλή όταν γνωρίζουμε ότι τα δεδομένα μας έχουν Gaussian κατανομή. Εξακολουθούν να μας επηρεάζουν οι ακραίες τιμές.

*Ενδεχομένως το κεντράρισμα να βοηθήσει ακόμα περισσότερο στο να μη μας επηρεάζουν οι ακραίες τιμές. Προτείνεται lecture 6 σελ.27

Πηγές:
https://towardsai.net/p/data-science/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff
https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html

----QA2_1----

MSE -> Προτιμάται σε προβλήματα παλινδρόμησης.

CE -> Προτιμάται σε προβλήματα ταξινόμησης.

Πηγές:
https://www.baeldung.com/cs/ml-loss-accuracy
https://vitalflux.com/mean-squared-error-vs-cross-entropy-loss-function/

----QA2_3----
Σίγουρα θέλουμε Σιγμοειδή. Η υπερβολική εφαπτομένη προτιμάται της λογιστικής ενώ η RELU χρησιμοποιείται πιο συχνά,
είναι πιο γρήγορη υπολογιστικά και δεν παρουσιάζει κορεσμό.

Πηγές:
lecture 6 σελ. 25-26
σημειώσεις σελ. 96-98
https://www.geeksforgeeks.org/activation-functions-neural-networks/
https://www.aitude.com/comparison-of-sigmoid-tanh-and-relu-activation-functions/

----QA2_4----
Softmax γιατί είναι κατάλληλη για ταξινόμηση. lecture 6 pg.5