----QA1_1----

Normalization -> Καλή όταν δε ξέρουμε την κατανομή των δεδομένω ή όταν αυτή δεν είναι Gaussian. Μειώνουμε την επίδραση των ακραίων τιμών.

Standarization -> Καλή όταν γνωρίζουμε ότι τα δεδομένα μας έχουν Gaussian κατανομή. Εξακολουθούν να μας επηρεάζουν οι ακραίες τιμές.

*Ενδεχομένως το κεντράρισμα να βοηθήσει ακόμα περισσότερο στο να μη μας επηρεάζουν οι ακραίες τιμές. Προτείνεται lecture 6 σελ.27

Πηγές:
https://towardsai.net/p/data-science/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff
https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html

----QA2_1----

MSE -> Προτιμάται σε προβλήματα παλινδρόμησης.

CE -> Προτιμάται σε προβλήματα ταξινόμησης.

Πηγές:
https://www.baeldung.com/cs/ml-loss-accuracy
https://vitalflux.com/mean-squared-error-vs-cross-entropy-loss-function/

----QA2_2----

πηγές:
https://www.enthought.com/blog/neural-network-output-layer/

----QA2_3----
Η υπερβολική εφαπτομένη προτιμάται της λογιστικής ενώ η RELU χρησιμοποιείται πιο συχνά,
είναι πιο γρήγορη υπολογιστικά και δεν παρουσιάζει κορεσμό.

Πηγές:
lecture 6 σελ. 25-26
σημειώσεις σελ. 96-98
https://www.geeksforgeeks.org/activation-functions-neural-networks/
https://www.aitude.com/comparison-of-sigmoid-tanh-and-relu-activation-functions/

----QA2_4----
Softmax γιατί είναι κατάλληλη για ταξινόμηση. lecture 6 pg.5

----QA3-----
https://www.jeremyjordan.me/nn-learning-rate/

----QA4-----
Πηγές:
https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c
https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization
https://towardsdatascience.com/regularization-techniques-for-neural-networks-379f5b4c9ac3
https://towardsdatascience.com/l1-vs-l2-regularization-in-machine-learning-differences-advantages-and-how-to-apply-them-in-72eb12f102b5
